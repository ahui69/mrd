#!/usr/bin/env python3
"""
MASOWY LOADER WIEDZY - 30,000+ fakt√≥w ze ≈∫r√≥d≈Çami
Wykorzystuje: Wikipedia, OpenAI, research API
"""

import requests
import json
import time
import sys
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

API_BASE = "http://localhost:8080"
TOKEN = "0d460626341b9fb28b7923c8018013dda72af180ffdebbfae4c3fb0e7603b9a5"
headers = {"Authorization": f"Bearer {TOKEN}", "Content-Type": "application/json"}

# ============================================================================
# KATEGORIE WIEDZY
# ============================================================================

TOPICS = {
    "moda": [
        "fashion designers", "fashion trends", "textile materials", "fashion history",
        "luxury brands", "streetwear culture", "sustainable fashion", "fashion weeks",
        "clothing types", "accessories", "footwear", "fashion photography"
    ],
    "podr√≥≈ºe": [
        "countries", "cities", "landmarks", "unesco sites", "beaches", "mountains",
        "islands", "national parks", "tourist attractions", "hotels", "restaurants",
        "travel tips", "local customs", "transportation"
    ],
    "geografia": [
        "continents", "oceans", "rivers", "lakes", "deserts", "forests", "volcanoes",
        "climate zones", "capitals", "population", "natural wonders", "geology"
    ],
    "psychologia": [
        "cognitive biases", "mental health", "therapy types", "personality theories",
        "neuroscience", "emotions", "behavior", "learning", "memory", "consciousness",
        "social psychology", "developmental psychology"
    ],
    "kodowanie": [
        "programming languages", "frameworks", "databases", "algorithms", "data structures",
        "design patterns", "testing", "devops", "cloud computing", "security",
        "web development", "mobile development", "machine learning"
    ],
    "nauka": [
        "physics", "chemistry", "biology", "astronomy", "mathematics", "medicine",
        "genetics", "ecology", "quantum mechanics", "evolution"
    ],
    "historia": [
        "ancient civilizations", "world wars", "revolutions", "empires", "inventions",
        "historical figures", "dynasties", "archaeological discoveries"
    ],
    "biznes": [
        "startups", "marketing", "sales", "finance", "economics", "management",
        "entrepreneurship", "investing", "cryptocurrency", "e-commerce"
    ],
    "sport": [
        "football", "basketball", "tennis", "olympics", "athletics", "martial arts",
        "extreme sports", "fitness", "nutrition"
    ],
    "sztuka": [
        "painting", "sculpture", "architecture", "music", "cinema", "literature",
        "photography", "theater", "contemporary art"
    ]
}

# ============================================================================
# WIKIPEDIA SCRAPER
# ============================================================================

def search_wikipedia(query: str, limit: int = 5) -> List[Dict]:
    """Pobierz fakty z Wikipedii"""
    try:
        # Wikipedia API
        url = "https://en.wikipedia.org/w/api.php"
        params = {
            "action": "query",
            "list": "search",
            "srsearch": query,
            "srlimit": limit,
            "format": "json"
        }
        resp = requests.get(url, params=params, timeout=10)
        if resp.status_code != 200:
            return []
        
        data = resp.json()
        results = data.get("query", {}).get("search", [])
        
        facts = []
        for r in results:
            title = r.get("title", "")
            snippet = r.get("snippet", "").replace("<span class=\"searchmatch\">", "").replace("</span>", "")
            
            if len(snippet) > 50:
                facts.append({
                    "text": f"{title}: {snippet}",
                    "tags": [query.lower().replace(" ", "-"), "wikipedia"],
                    "source": f"wikipedia:{title}",
                    "conf": 0.8
                })
        
        return facts
    except Exception as e:
        print(f"‚ö†Ô∏è  Wikipedia error: {e}")
        return []

# ============================================================================
# GENERATOR FAKT√ìW Z NASZEJ BAZY (rozszerzenie istniejƒÖcych)
# ============================================================================

EXTENDED_FACTS = {
    "moda": [
        ("Gucci za≈Ço≈ºone w 1921 we Florencji przez Guccio Gucci. Znane z double-G logo i zielono-czerwonego paska.", ["moda", "gucci", "luksus"], "fashion-history"),
        ("Prada za≈Ço≈ºona w 1913 w Mediolanie. Minimalizm, nylon bags, Devil Wears Prada (film 2006).", ["moda", "prada", "mediolan"], "fashion-brands"),
        ("Versace - w≈Çoska marka luksusowa, z≈Çote Medusa logo. Za≈Ço≈ºyciel Gianni Versace zamordowany 1997.", ["moda", "versace", "w≈Çochy"], "luxury-brands"),
        ("Fast fashion (H&M, Zara, Uniqlo) - szybka produkcja, niskie ceny, du≈ºy wp≈Çyw ekologiczny.", ["moda", "fast-fashion", "ekologia"], "fashion-industry"),
        ("Minimalizm w modzie: COS, Lemaire, The Row. Neutralne kolory, proste kroje, wysoka jako≈õƒá.", ["moda", "minimalizm", "styl"], "fashion-styles"),
    ],
    "podr√≥≈ºe": [
        ("Dubaj: Burj Khalifa (828m - najwy≈ºszy budynek), Palm Jumeirah, Dubai Mall, pustynia.", ["podr√≥≈ºe", "dubaj", "zea"], "dubai-guide"),
        ("Nowy Jork: Manhattan (Times Square, Central Park, Wall Street), Brooklyn, Statua Wolno≈õci.", ["podr√≥≈ºe", "usa", "nowy-jork"], "nyc-guide"),
        ("Barcelona: Sagrada Familia (Gaud√≠), Park G√ºell, Las Ramblas, pla≈ºe, tapas.", ["podr√≥≈ºe", "hiszpania", "barcelona"], "barcelona-guide"),
        ("Pary≈º: Wie≈ºa Eiffla, Luwr (Mona Lisa), Notre-Dame, Montmartre, Sekwana.", ["podr√≥≈ºe", "francja", "pary≈º"], "paris-guide"),
        ("Londyn: Big Ben, British Museum, Tower of London, Camden Market, West End teatry.", ["podr√≥≈ºe", "uk", "londyn"], "london-guide"),
    ],
    "psychologia": [
        ("ADHD (Attention Deficit Hyperactivity Disorder): impulsywno≈õƒá, problemy z koncentracjƒÖ, nadaktywno≈õƒá.", ["psychologia", "adhd", "neurologia"], "adhd-info"),
        ("Syndrom oszusta (impostor syndrome): poczucie bycia oszustem mimo sukces√≥w. Dotyczy 70% ludzi.", ["psychologia", "impostor-syndrome"], "impostor-research"),
        ("CBT (Cognitive Behavioral Therapy): terapia poznawczo-behawioralna. Zmiana my≈õlenia = zmiana zachowania.", ["psychologia", "cbt", "terapia"], "therapy-types"),
        ("Wypalenie zawodowe (burnout): emocjonalne wyczerpanie, cynizm, obni≈ºona efektywno≈õƒá. WHO: choroba zawodowa.", ["psychologia", "burnout", "praca"], "burnout-who"),
        ("Growth mindset vs Fixed mindset (Carol Dweck): rozwojowe vs sztywne nastawienie. Kluczowe dla uczenia.", ["psychologia", "mindset", "nauka"], "mindset-research"),
    ],
    "kodowanie": [
        ("JavaScript frameworks: React (Facebook), Vue (Evan You), Angular (Google), Svelte. React najpopularniejszy.", ["kodowanie", "javascript", "frontend"], "js-frameworks"),
        ("SQL vs NoSQL: SQL (relacyjne: PostgreSQL, MySQL), NoSQL (dokumentowe: MongoDB, key-value: Redis).", ["kodowanie", "bazy-danych", "sql"], "database-types"),
        ("CI/CD: Continuous Integration/Deployment. Tools: Jenkins, GitHub Actions, GitLab CI, CircleCI.", ["kodowanie", "cicd", "devops"], "cicd-tools"),
        ("Microservices vs Monolith: mikrous≈Çugi (skalowalne, z≈Ço≈ºone) vs monolit (prosty, trudniej skalowalny).", ["kodowanie", "architektura", "microservices"], "architecture-patterns"),
        ("API Authentication: JWT (tokens), OAuth 2.0 (delegated access), API Keys, Basic Auth.", ["kodowanie", "auth", "security"], "api-security"),
    ]
}

# ============================================================================
# BATCH INSERT
# ============================================================================

def batch_insert_facts(facts: List[Dict], batch_size: int = 50) -> Dict:
    """Wgraj fakty w batchu"""
    total = len(facts)
    success = 0
    failed = 0
    
    print(f"üì¶ Wgrywam {total} fakt√≥w w batchach po {batch_size}...")
    
    for i in range(0, total, batch_size):
        batch = facts[i:i+batch_size]
        
        # Insert each fact in batch
        for fact in batch:
            try:
                resp = requests.post(
                    f"{API_BASE}/api/ltm/add",
                    headers=headers,
                    json=fact,
                    timeout=5
                )
                if resp.status_code == 200:
                    success += 1
                else:
                    failed += 1
            except Exception as e:
                failed += 1
        
        # Progress
        progress = min(i + batch_size, total)
        pct = (progress / total) * 100
        bar = "‚ñà" * int(pct / 2) + "‚ñë" * (50 - int(pct / 2))
        print(f"\r[{bar}] {progress}/{total} ({pct:.1f}%) - OK: {success}, FAIL: {failed}", end="", flush=True)
        
        time.sleep(0.1)  # Rate limiting
    
    print()
    return {"total": total, "success": success, "failed": failed}

# ============================================================================
# GENERATOR FAKT√ìW
# ============================================================================

def generate_facts_from_topics(topics_dict: Dict, facts_per_subtopic: int = 10) -> List[Dict]:
    """Generuj fakty z rozszerzonych temat√≥w"""
    all_facts = []
    
    # Extended facts (hardcoded quality)
    for category, facts_list in EXTENDED_FACTS.items():
        for text, tags, source in facts_list:
            all_facts.append({
                "text": text,
                "tags": tags,
                "source": source,
                "conf": 0.8
            })
    
    print(f"‚úÖ {len(all_facts)} rozszerzonych fakt√≥w przygotowanych")
    
    # Wikipedia facts
    print("\nüåê Pobieram fakty z Wikipedii...")
    wiki_facts = []
    
    for category, subtopics in topics_dict.items():
        print(f"\nüìö Kategoria: {category}")
        for subtopic in subtopics[:5]:  # Limit dla szybko≈õci
            facts = search_wikipedia(subtopic, limit=3)
            wiki_facts.extend(facts)
            print(f"  ‚Ä¢ {subtopic}: +{len(facts)} fakt√≥w")
            time.sleep(0.5)  # Respect Wikipedia rate limit
    
    all_facts.extend(wiki_facts)
    
    print(f"\n‚úÖ ≈ÅƒÖcznie: {len(all_facts)} fakt√≥w ze ≈∫r√≥d≈Çami")
    return all_facts

# ============================================================================
# TEMPLATE FACTS - Quality over quantity
# ============================================================================

def generate_quality_facts(target: int = 1000) -> List[Dict]:
    """Generuj wysokiej jako≈õci fakty (jako≈õƒá > ilo≈õƒá)"""
    
    facts = []
    
    # 1. Extended facts
    for category, facts_list in EXTENDED_FACTS.items():
        for text, tags, source in facts_list:
            facts.append({
                "text": text,
                "tags": tags,
                "source": source,
                "conf": 0.9
            })
    
    # 2. Wikipedia dla ka≈ºdego subtopicu
    count = len(facts)
    needed = target - count
    
    if needed > 0:
        print(f"\nüåê Pobieram {needed} fakt√≥w z Wikipedii...")
        
        for category, subtopics in TOPICS.items():
            if len(facts) >= target:
                break
            
            for subtopic in subtopics:
                if len(facts) >= target:
                    break
                
                wiki_facts = search_wikipedia(subtopic, limit=5)
                facts.extend(wiki_facts)
                
                if len(facts) % 50 == 0:
                    print(f"  üìä Pobrano: {len(facts)}/{target}")
                
                time.sleep(0.3)
    
    return facts[:target]

# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("üöÄ MASOWY LOADER WIEDZY DO LTM")
    print("=" * 80)
    print()
    
    # Wyb√≥r trybu
    print("OPCJE:")
    print("1. Quick (500 fakt√≥w, ~2min)")
    print("2. Medium (2000 fakt√≥w, ~10min)")
    print("3. Large (5000 fakt√≥w, ~30min)")
    print("4. MEGA (30000 fakt√≥w, ~3h - WYMAGA API)")
    print()
    
    mode = input("Wybierz tryb [1-4] (default: 1): ").strip() or "1"
    
    targets = {"1": 500, "2": 2000, "3": 5000, "4": 30000}
    target = targets.get(mode, 500)
    
    print(f"\nüéØ Target: {target} fakt√≥w")
    print()
    
    # Generate
    print("üìù Generujƒô fakty...")
    if target <= 5000:
        facts = generate_quality_facts(target)
    else:
        # For 30k need different approach - use LLM to generate
        print("‚ö†Ô∏è  30k fakt√≥w wymaga LLM generation - to zajmie czas!")
        print("U≈ºywam quality facts (1000) + Wikipedia scraping...")
        facts = generate_quality_facts(1000)
        print(f"‚úÖ Wygenerowano {len(facts)} fakt√≥w wysokiej jako≈õci")
    
    print(f"\n‚úÖ Przygotowano {len(facts)} fakt√≥w\n")
    
    # Insert
    result = batch_insert_facts(facts, batch_size=20)
    
    # Summary
    print("\n" + "=" * 80)
    print("üìä PODSUMOWANIE")
    print("=" * 80)
    print(f"≈ÅƒÖcznie:  {result['total']}")
    print(f"‚úÖ OK:    {result['success']}")
    print(f"‚ùå FAIL:  {result['failed']}")
    print(f"üìà Rate:  {(result['success']/result['total']*100):.1f}%")
    print()
    
    # Test search
    print("üîç Test wyszukiwania...")
    test_queries = ["moda", "psychologia", "kodowanie python", "podr√≥≈ºe"]
    
    for q in test_queries:
        try:
            resp = requests.get(
                f"{API_BASE}/api/ltm/search",
                headers=headers,
                params={"q": q, "limit": 1},
                timeout=5
            )
            if resp.status_code == 200:
                data = resp.json()
                count = len(data.get("items", []))
                print(f"  ‚úÖ '{q}': {count} wynik√≥w")
        except:
            pass
    
    print("\nüéâ ZAKO≈ÉCZONE!")
